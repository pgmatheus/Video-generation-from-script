{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 27/27 [00:09<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.532152652740479\n",
      "delete\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" from scripts_2.high_res_img import high_img\n",
    "import torch\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "high_img_f = high_img(4)\n",
    "\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "\n",
    "for file in tqdm(os.listdir('./test_inp/')):\n",
    "\n",
    "    high_img_f.run({\n",
    "    'face': False, # reconstruct face\n",
    "    'path': f'./test_inp/{file}', # image path\n",
    "    'w_i': 480, # width to be resized \n",
    "    'h_i': 270, # height to be resized\n",
    "    'w_o': 1920, # width to be resized \n",
    "    'h_o': 1080, # height to be resized    \n",
    "    'out_path': f'./test/{file}', # image output location\n",
    "    'return_out': 'nothing', # return the data to use later\n",
    "    'show_image': False, # show output\n",
    "}) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "del high_img_f\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n"
     ]
    }
   ],
   "source": [
    "\"\"\" from scripts_2.high_res_img import high_img\n",
    "import torch\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "high_img_f = high_img(4)\n",
    "\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "\n",
    "for file in tqdm(os.listdir('./test_inp/')):\n",
    "\n",
    "    high_img_f.run({\n",
    "    'face': False, # reconstruct face\n",
    "    'path': f'./test_inp/{file}', # image path\n",
    "    'w_i': 480, # width to be resized \n",
    "    'h_i': 270, # height to be resized\n",
    "    'w_o': 1920, # width to be resized \n",
    "    'h_o': 1080, # height to be resized    \n",
    "    'out_path': f'./test/{file}', # image output location\n",
    "    'return_out': 'nothing', # return the data to use later\n",
    "    'show_image': False, # show output\n",
    "}) \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "del high_img_f\n",
    "print(torch.cuda.memory_allocated())\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips\n",
    "\n",
    "# Load the video and audio files\n",
    "video = VideoFileClip(\"f:/f.mp4\")\n",
    "audio = AudioFileClip(\"f:/f.wav\")\n",
    "\n",
    "# Create an audio clip that is the same length as the video\n",
    "audio = audio.set_duration(video.duration)\n",
    "\n",
    "# Merge the video and audio\n",
    "final_clip = video.set_audio(audio)\n",
    "\n",
    "# Write the final clip to a file\n",
    "final_clip.write_videofile(\"f:/m.mp4\", codec=\"libx264\")\n",
    "# Load the two images \"\"\"\n",
    "\n",
    "\"\"\" def resize_image(data):\n",
    "  resized_img = tf.image.resize(\n",
    "  images=data,\n",
    "  # size=[270, 480],     \n",
    "  size=[1080, 1920], \n",
    "  method=tf.image.ResizeMethod.BILINEAR,\n",
    "  preserve_aspect_ratio=False,\n",
    "  antialias=True,\n",
    "  )    \n",
    "  return resized_img\n",
    "\n",
    "img = cv2.imread(\"./z.png\")\n",
    "img = resize_image(img).numpy()\n",
    "#img = cv2.resize(img,(1920,1080))\n",
    "\n",
    "cv2.imwrite(\"007_ma.png\",img) \"\"\"\n",
    "\n",
    "\"\"\" import cv2\n",
    "s_img = cv2.imread(\"F:/gg/templates/hq/a globe inside a jar/0000002/output_0000.png\")\n",
    "s_img = cv2.resize(s_img,(672,378))\n",
    "l_img = cv2.imread(\"D:\\\\Deletar\\\\p_gen\\\\john test\\\\merged\\\\output_0000.png\")\n",
    "x_offset=1150\n",
    "y_offset = 300\n",
    "l_img[y_offset:y_offset+s_img.shape[0], x_offset:x_offset+s_img.shape[1]] = s_img\n",
    "\n",
    "cv2.imwrite('./test.png', l_img) \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" def merge_gg(s_img,l_img,output,x_offset=1150,y_offset = 300):\n",
    "    s_img = cv2.imread(s_img)\n",
    "    s_img = cv2.resize(s_img,(672,378))\n",
    "    l_img = cv2.imread(l_img)\n",
    "    l_img[y_offset:y_offset+s_img.shape[0], x_offset:x_offset+s_img.shape[1]] = s_img\n",
    "    cv2.imwrite(output, l_img)\n",
    "\n",
    "for img_path in os.listdir('D:\\\\Deletar\\\\p_gen\\\\john test\\\\merged'):\n",
    "    if img_path[-3:] == 'png':\n",
    "        merge_gg(f\"F:/gg/templates/hq/a globe inside a\n",
    "        jar/0000002/{img_path}\",f\"D:\\\\Deletar\\\\p_gen\\\\john\n",
    "        test\\\\merged\\\\{img_path}\",f\"./test/{img_path}\") \"\"\"\n",
    "\n",
    "print('q')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cb0dd519e46c84f14fc2d0d6172fde6407ecb78b6d95b871c78761f2e6e52fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
