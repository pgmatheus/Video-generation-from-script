{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' if __name__ == \\'__main__\\':\\n    argparser = argparse.ArgumentParser()\\n    argparser.add_argument(\"--img_path\", type=str, default=None, help=\"path of the input image ( .jpg ), preprocessed by image_preprocess.py\")\\n    argparser.add_argument(\"--audio_path\", type=str, default=None, help=\"path of the input audio ( .wav )\")\\n    argparser.add_argument(\"--phoneme_path\", type=str, default=None, help=\"path of the input phoneme. It should be note that the phoneme must be consistent with the input audio\")\\n    argparser.add_argument(\"--save_dir\", type=str, default=\"samples/results\", help=\"path of the output video\")\\n    args = argparser.parse_args()\\n\\n    phoneme = parse_phoneme_file(args.phoneme_path)\\n    test_with_input_audio_and_image(args.img_path,args.audio_path,phoneme,config.GENERATOR_CKPT,config.AUDIO2POSE_CKPT,args.save_dir) '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from models.generator import OcclusionAwareGenerator\n",
    "from models.keypoint_detector import KPDetector\n",
    "import argparse\n",
    "import imageio\n",
    "from models.util import draw_annotation_box\n",
    "from models.transformer import Audio2kpTransformer\n",
    "from scipy.io import wavfile\n",
    "from tools.interface import read_img,get_img_pose,get_pose_from_audio,get_audio_feature_from_audio,\\\n",
    "    parse_phoneme_file,load_ckpt\n",
    "import config\n",
    "\n",
    "def normalize_kp(kp_source, kp_driving, kp_driving_initial,\n",
    "                 use_relative_movement=True, use_relative_jacobian=True):\n",
    "\n",
    "    kp_new = {k: v for k, v in kp_driving.items()}\n",
    "    if use_relative_movement:\n",
    "        kp_value_diff = (kp_driving['value'] - kp_driving_initial['value'])\n",
    "        # kp_value_diff *= adapt_movement_scale\n",
    "        kp_new['value'] = kp_value_diff + kp_source['value']\n",
    "\n",
    "        if use_relative_jacobian:\n",
    "            jacobian_diff = torch.matmul(kp_driving['jacobian'], torch.inverse(kp_driving_initial['jacobian']))\n",
    "            kp_new['jacobian'] = torch.matmul(jacobian_diff, kp_source['jacobian'])\n",
    "\n",
    "    return kp_new\n",
    "\n",
    "\n",
    "def test_with_input_audio_and_image(img_path, audio_path,phs, generator_ckpt, audio2pose_ckpt, save_dir=\"samples/results\"):\n",
    "    with open(\"config_file/vox-256.yaml\") as f:\n",
    "        # config = yaml.load(f)\n",
    "        config = yaml.load(f, Loader=SafeLoader)\n",
    "    # temp_audio = audio_path\n",
    "    # print(audio_path)\n",
    "    cur_path = os.getcwd()\n",
    "\n",
    "    sr,_ = wavfile.read(audio_path)\n",
    "    if sr!=16000:\n",
    "        temp_audio = os.path.join(cur_path,\"samples\",\"temp.wav\")\n",
    "        command = \"ffmpeg -y -i %s -async 1 -ac 1 -vn -acodec pcm_s16le -ar 16000 %s\" % (audio_path, temp_audio)\n",
    "        os.system(command)\n",
    "    else:\n",
    "        temp_audio = audio_path\n",
    "\n",
    "\n",
    "    opt = argparse.Namespace(**yaml.load(open(\"config_file/audio2kp.yaml\"), Loader=SafeLoader))\n",
    "\n",
    "    img = read_img(img_path).cuda()\n",
    "\n",
    "    first_pose = get_img_pose(img_path)#.cuda()\n",
    "\n",
    "    audio_feature = get_audio_feature_from_audio(temp_audio)\n",
    "    frames = len(audio_feature) // 4\n",
    "    frames = min(frames,len(phs[\"phone_list\"]))\n",
    "\n",
    "    tp = np.zeros([256, 256], dtype=np.float32)\n",
    "    draw_annotation_box(tp, first_pose[:3], first_pose[3:])\n",
    "    tp = torch.from_numpy(tp).unsqueeze(0).unsqueeze(0).cuda()\n",
    "    ref_pose = get_pose_from_audio(tp, audio_feature, audio2pose_ckpt)\n",
    "    torch.cuda.empty_cache()\n",
    "    trans_seq = ref_pose[:, 3:]\n",
    "    rot_seq = ref_pose[:, :3]\n",
    "\n",
    "\n",
    "\n",
    "    audio_seq = audio_feature#[40:]\n",
    "    ph_seq = phs[\"phone_list\"]\n",
    "\n",
    "\n",
    "    ph_frames = []\n",
    "    audio_frames = []\n",
    "    pose_frames = []\n",
    "    name_len = frames\n",
    "\n",
    "    pad = np.zeros((4, audio_seq.shape[1]), dtype=np.float32)\n",
    "\n",
    "    for rid in range(0, frames):\n",
    "        ph = []\n",
    "        audio = []\n",
    "        pose = []\n",
    "        for i in range(rid - opt.num_w, rid + opt.num_w + 1):\n",
    "            if i < 0:\n",
    "                rot = rot_seq[0]\n",
    "                trans = trans_seq[0]\n",
    "                ph.append(31)\n",
    "                audio.append(pad)\n",
    "            elif i >= name_len:\n",
    "                ph.append(31)\n",
    "                rot = rot_seq[name_len - 1]\n",
    "                trans = trans_seq[name_len - 1]\n",
    "                audio.append(pad)\n",
    "            else:\n",
    "                ph.append(ph_seq[i])\n",
    "                rot = rot_seq[i]\n",
    "                trans = trans_seq[i]\n",
    "                audio.append(audio_seq[i * 4:i * 4 + 4])\n",
    "            tmp_pose = np.zeros([256, 256])\n",
    "            draw_annotation_box(tmp_pose, np.array(rot), np.array(trans))\n",
    "            pose.append(tmp_pose)\n",
    "\n",
    "        ph_frames.append(ph)\n",
    "        audio_frames.append(audio)\n",
    "        pose_frames.append(pose)\n",
    "\n",
    "    audio_f = torch.from_numpy(np.array(audio_frames,dtype=np.float32)).unsqueeze(0)\n",
    "    poses = torch.from_numpy(np.array(pose_frames, dtype=np.float32)).unsqueeze(0)\n",
    "    ph_frames = torch.from_numpy(np.array(ph_frames)).unsqueeze(0)\n",
    "    bs = audio_f.shape[1]\n",
    "    predictions_gen = []\n",
    "\n",
    "    kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                             **config['model_params']['common_params'])\n",
    "    generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "    kp_detector = kp_detector.cuda()\n",
    "    generator = generator.cuda()\n",
    "\n",
    "    ph2kp = Audio2kpTransformer(opt).cuda()\n",
    "\n",
    "    load_ckpt(generator_ckpt, kp_detector=kp_detector, generator=generator,ph2kp=ph2kp)\n",
    "\n",
    "\n",
    "    ph2kp.eval()\n",
    "    generator.eval()\n",
    "    kp_detector.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frame_idx in range(bs):\n",
    "            t = {}\n",
    "\n",
    "            t[\"audio\"] = audio_f[:, frame_idx].cuda()\n",
    "            t[\"pose\"] = poses[:, frame_idx].cuda()\n",
    "            t[\"ph\"] = ph_frames[:,frame_idx].cuda()\n",
    "            t[\"id_img\"] = img\n",
    "\n",
    "            kp_gen_source = kp_detector(img, True)\n",
    "\n",
    "            gen_kp = ph2kp(t,kp_gen_source)\n",
    "            if frame_idx == 0:\n",
    "                drive_first = gen_kp\n",
    "\n",
    "            norm = normalize_kp(kp_source=kp_gen_source, kp_driving=gen_kp, kp_driving_initial=drive_first)\n",
    "            out_gen = generator(img, kp_source=kp_gen_source, kp_driving=norm)\n",
    "\n",
    "            predictions_gen.append(\n",
    "                (np.transpose(out_gen['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0] * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "    log_dir = save_dir\n",
    "    os.makedirs(os.path.join(log_dir, \"temp\"),exist_ok=True)\n",
    "\n",
    "    f_name = os.path.basename(img_path)[:-4] + \"_\" + os.path.basename(audio_path)[:-4] + \".mp4\"\n",
    "    # kwargs = {'duration': 1. / 25.0}\n",
    "    video_path = os.path.join(log_dir, \"temp\", f_name)\n",
    "    print(\"save video to: \", video_path)\n",
    "    imageio.mimsave(video_path, predictions_gen, fps=25.0)\n",
    "\n",
    "    # audio_path = os.path.join(audio_dir, x['name'][0].replace(\".mp4\", \".wav\"))\n",
    "    save_video = os.path.join(log_dir, f_name)\n",
    "    cmd = r'ffmpeg -y -i \"%s\" -i \"%s\" -vcodec copy \"%s\"' % (video_path, audio_path, save_video)\n",
    "    os.system(cmd)\n",
    "    os.remove(video_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" if __name__ == '__main__':\n",
    "    argparser = argparse.ArgumentParser()\n",
    "    argparser.add_argument(\"--img_path\", type=str, default=None, help=\"path of the input image ( .jpg ), preprocessed by image_preprocess.py\")\n",
    "    argparser.add_argument(\"--audio_path\", type=str, default=None, help=\"path of the input audio ( .wav )\")\n",
    "    argparser.add_argument(\"--phoneme_path\", type=str, default=None, help=\"path of the input phoneme. It should be note that the phoneme must be consistent with the input audio\")\n",
    "    argparser.add_argument(\"--save_dir\", type=str, default=\"samples/results\", help=\"path of the output video\")\n",
    "    args = argparser.parse_args()\n",
    "\n",
    "    phoneme = parse_phoneme_file(args.phoneme_path)\n",
    "    test_with_input_audio_and_image(args.img_path,args.audio_path,phoneme,config.GENERATOR_CKPT,config.AUDIO2POSE_CKPT,args.save_dir) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\" test_with_input_audio_and_image(img_path, audio_path,phs, generator_ckpt, audio2pose_ckpt, save_dir=\"samples/results\") \"\"\"\n",
    "\n",
    "img_path = \".\\\\samples\\\\imgs\\\\trump2.jpg\" \n",
    "audio_path = \".\\\\samples\\\\audios\\\\trump_strong.wav\"\n",
    "generator_ckpt = \".\\\\checkpoints\\\\generator.ckpt\"\n",
    "audio2pose_ckpt = \".\\\\checkpoints\\\\audio2pose.ckpt\"\n",
    "phs = \".\\\\models\\\\phindex.json\"\n",
    "phs = \".\\\\samples\\\\phonemes\\\\trump_strong.json\"\n",
    "\n",
    "test_with_input_audio_and_image(img_path, audio_path,phs, generator_ckpt, audio2pose_ckpt, save_dir=\"samples/results\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cb0dd519e46c84f14fc2d0d6172fde6407ecb78b6d95b871c78761f2e6e52fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
