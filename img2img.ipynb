{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from diffusers import DiffusionPipeline\n",
    "import os\n",
    "from typing import List\n",
    "from cog import BasePredictor, Input, Path\n",
    "import torch\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    PNDMScheduler,\n",
    "    LMSDiscreteScheduler,\n",
    "    DDIMScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "EXTERNAL = False\n",
    "\n",
    "# if download\n",
    "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "# os.makedirs(MODEL_CACHE, exist_ok=True)\n",
    "#local storage\n",
    "model_id = \"./diffusers-cache/models--stabilityai--stable-diffusion-2-1/snapshots/36a01dc742066de2e8c91e7cf0b8f6b53ef53da1/\"\n",
    "\n",
    "MODEL_CACHE = \"diffusers-cache\"\n",
    "scheduler='DPMSolverMultistep'\n",
    "\n",
    "\n",
    "# download heigths\n",
    "#pipe = DiffusionPipeline.from_pretrained(\n",
    "#    model_id,\n",
    "#    cache_dir=MODEL_CACHE,\n",
    "#)\n",
    "\n",
    "if EXTERNAL:\n",
    "  %store -r WIDTH\n",
    "  %store -r HEIGHT\n",
    "  %store -r TEXT\n",
    "  %store -r VISUALIZE\n",
    "  %store -r SAVE_FOLDER\n",
    "  %store -r PROJECT_NAME\n",
    "  %store -r SHOW_OUTPUT\n",
    "  %store -r IMG_NUMBER\n",
    "  %store -r THUMBNAIL\n",
    "  %store -r IMG_NUMBER_IMG2IMG\n",
    "\n",
    "else:\n",
    "  WIDTH = 768\n",
    "  HEIGHT = 768\n",
    "  VISUALIZE = True  \n",
    "  SAVE_FOLDER = \"D:\\\\Deletar\\\\p_gen\"\n",
    "  PROJECT_NAME = \"THE FELLOWSHIP OF THE RING\"\n",
    "  STYLES_FOLDER = \".\\\\styles\"\n",
    "  SHOW_OUTPUT = True\n",
    "  IMG_NUMBER = 2\n",
    "  IMG_NUMBER_IMG2IMG = 5\n",
    "\n",
    "\n",
    "txt2img_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            cache_dir=MODEL_CACHE,\n",
    "            local_files_only=True,\n",
    "        ).to(\"cuda\")    \n",
    "\n",
    "txt2img_pipe.set_use_memory_efficient_attention_xformers(True)\n",
    "\n",
    "img2img_pipe = StableDiffusionImg2ImgPipeline(\n",
    "            vae=txt2img_pipe.vae,\n",
    "            text_encoder=txt2img_pipe.text_encoder,\n",
    "            tokenizer=txt2img_pipe.tokenizer,\n",
    "            unet=txt2img_pipe.unet,\n",
    "            scheduler=txt2img_pipe.scheduler,\n",
    "            safety_checker=txt2img_pipe.safety_checker,\n",
    "            feature_extractor=txt2img_pipe.feature_extractor,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "img2img_pipe.set_use_memory_efficient_attention_xformers(True)\n",
    "pipe = img2img_pipe\n",
    "project_folder = f\"{SAVE_FOLDER}\\\\{PROJECT_NAME}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scheduler(name, config):\n",
    "    return {\n",
    "        \"PNDM\": PNDMScheduler.from_config(config),\n",
    "        \"KLMS\": LMSDiscreteScheduler.from_config(config),\n",
    "        \"DDIM\": DDIMScheduler.from_config(config),\n",
    "        \"K_EULER\": EulerDiscreteScheduler.from_config(config),\n",
    "        \"K_EULER_ANCESTRAL\": EulerAncestralDiscreteScheduler.from_config(config),\n",
    "        \"DPMSolverMultistep\": DPMSolverMultistepScheduler.from_config(config),\n",
    "    }[name]\n",
    "    \n",
    "pipe.scheduler = make_scheduler(scheduler, pipe.scheduler.config)\n",
    "\n",
    "\"\"\" MODEL_ID = \"stabilityai/stable-diffusion-2-1\"\n",
    "MODEL_CACHE = \"diffusers-cache\"\n",
    "\n",
    "MODEL_ID = \"./diffusers-cache/models--stabilityai--stable-diffusion-2-1/snapshots/36a01dc742066de2e8c91e7cf0b8f6b53ef53da1/\" \"\"\"\n",
    "\n",
    "class Predictor(BasePredictor):\n",
    "    def setup(self):       \n",
    "        \"\"\" print(\"Loading pipeline...\")\n",
    "        self.txt2img_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            cache_dir=MODEL_CACHE,\n",
    "            local_files_only=True,\n",
    "        ).to(\"cuda\")\n",
    "        self.img2img_pipe = StableDiffusionImg2ImgPipeline(\n",
    "            vae=self.txt2img_pipe.vae,\n",
    "            text_encoder=self.txt2img_pipe.text_encoder,\n",
    "            tokenizer=self.txt2img_pipe.tokenizer,\n",
    "            unet=self.txt2img_pipe.unet,\n",
    "            scheduler=self.txt2img_pipe.scheduler,\n",
    "            safety_checker=self.txt2img_pipe.safety_checker,\n",
    "            feature_extractor=self.txt2img_pipe.feature_extractor,\n",
    "        ).to(\"cuda\") \"\"\"\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(\n",
    "        self,\n",
    "        prompt: str = Input(\n",
    "            description=\"Input prompt\",\n",
    "            default=\"A fantasy landscape, trending on artstation\",\n",
    "        ),\n",
    "        negative_prompt: str = Input(\n",
    "            description=\"The prompt NOT to guide the image generation. Ignored when not using guidance\",\n",
    "            default=None,\n",
    "        ),\n",
    "        image: Path = Input(\n",
    "            description=\"Inital image to generate variations of.\",\n",
    "        ),\n",
    "        width: int = Input(\n",
    "            description=\"Width of output image. Maximum size is 1024x768 or 768x1024 because of memory limits\",\n",
    "            choices=[128, 256, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960, 1024],\n",
    "            default=512,\n",
    "        ),\n",
    "        height: int = Input(\n",
    "            description=\"Height of output image. Maximum size is 1024x768 or 768x1024 because of memory limits\",\n",
    "            choices=[128, 256, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960, 1024],\n",
    "            default=512,\n",
    "        ),\n",
    "        prompt_strength: float = Input(\n",
    "            description=\"Prompt strength when providing the image. 1.0 corresponds to full destruction of information in init image\",\n",
    "            default=0.8,\n",
    "        ),\n",
    "        num_outputs: int = Input(\n",
    "            description=\"Number of images to output. Higher number of outputs may OOM.\",\n",
    "            ge=1,\n",
    "            le=8,\n",
    "            default=1,\n",
    "        ),\n",
    "        num_inference_steps: int = Input(\n",
    "            description=\"Number of denoising steps\", ge=1, le=500, default=25\n",
    "        ),\n",
    "        guidance_scale: float = Input(\n",
    "            description=\"Scale for classifier-free guidance\", ge=1, le=20, default=7.5\n",
    "        ),\n",
    "        scheduler: str = Input(\n",
    "            default=\"DPMSolverMultistep\",\n",
    "            choices=[\"DDIM\", \"K_EULER\", \"DPMSolverMultistep\", \"K_EULER_ANCESTRAL\", \"PNDM\", \"KLMS\"],\n",
    "            description=\"Choose a scheduler.\",\n",
    "        ),\n",
    "        seed: int = Input(\n",
    "            description=\"Random seed. Leave blank to randomize the seed\", default=None\n",
    "        ),\n",
    "        output_location: str = Input(\n",
    "            description=\"Output location\", default=\"./\"),\n",
    "        index: int = Input(\n",
    "            description=\"Index\", default=\"0\"),\n",
    "    ) -> List[Path]:\n",
    "       \n",
    "        if seed is None:\n",
    "            seed = int.from_bytes(os.urandom(2), \"big\")\n",
    "        # print(f\"Using seed: {seed}\")\n",
    "\n",
    "        \n",
    "        extra_kwargs = {\n",
    "            \"image\": Image.open(image).convert(\"RGB\"),\n",
    "            \"strength\": prompt_strength,\n",
    "        }\n",
    "        \n",
    "\n",
    "        generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "        output = pipe(\n",
    "            # prompt=[prompt] * num_outputs if prompt is not None else None,\n",
    "            prompt= prompt,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            **extra_kwargs,\n",
    "        )\n",
    "\n",
    "        output_paths = []\n",
    "        for i, sample in enumerate(output.images):\n",
    "            output_path = f\"{output_location}\\\\result_a{index}.jpg\"\n",
    "            sample.save(output_path)\n",
    "            # output_paths.append(Path(output_path))\n",
    "\n",
    "        return output_paths\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "run_scr = Predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c456b14dbb7d4f36bfdca3f41d00a97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d57fa667449407b97c70d2faefea3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7967d29ee725438fabf6612a9e944d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3e59d68e364b448691922722bec5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f56ea2056c4741a20038d8eb9408e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66af787c2c014527b1a5cfa5a194774a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d6d684521740afb741fef66b40deb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57add0e6d5f342699650ec905a6cb8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m---> 25\u001b[0m         run_scr\u001b[39m.\u001b[39;49mpredict(prompt \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m(((beautiful))), detailed, (((young woman))), high definition, realistic\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m     26\u001b[0m         image\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./c.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m, seed \u001b[39m=\u001b[39;49m \u001b[39mint\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_bytes(os\u001b[39m.\u001b[39;49murandom(\u001b[39m2\u001b[39;49m), \u001b[39m\"\u001b[39;49m\u001b[39mbig\u001b[39;49m\u001b[39m\"\u001b[39;49m), output_location \u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./random\u001b[39;49m\u001b[39m\"\u001b[39;49m , index \u001b[39m=\u001b[39;49m i,\n\u001b[0;32m     27\u001b[0m         prompt_strength\u001b[39m=\u001b[39;49m \u001b[39m0.6\u001b[39;49m, guidance_scale \u001b[39m=\u001b[39;49m \u001b[39m7.5\u001b[39;49m, num_inference_steps\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[2], line 102\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[1;34m(self, prompt, negative_prompt, image, width, height, prompt_strength, num_outputs, num_inference_steps, guidance_scale, scheduler, seed, output_location, index)\u001b[0m\n\u001b[0;32m     95\u001b[0m extra_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m     96\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m: Image\u001b[39m.\u001b[39mopen(image)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     97\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstrength\u001b[39m\u001b[39m\"\u001b[39m: prompt_strength,\n\u001b[0;32m     98\u001b[0m }\n\u001b[0;32m    101\u001b[0m generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mGenerator(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m--> 102\u001b[0m output \u001b[39m=\u001b[39m pipe(\n\u001b[0;32m    103\u001b[0m     \u001b[39m# prompt=[prompt] * num_outputs if prompt is not None else None,\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     prompt\u001b[39m=\u001b[39m prompt,\n\u001b[0;32m    105\u001b[0m     guidance_scale\u001b[39m=\u001b[39mguidance_scale,\n\u001b[0;32m    106\u001b[0m     generator\u001b[39m=\u001b[39mgenerator,\n\u001b[0;32m    107\u001b[0m     num_inference_steps\u001b[39m=\u001b[39mnum_inference_steps,\n\u001b[0;32m    108\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kwargs,\n\u001b[0;32m    109\u001b[0m )\n\u001b[0;32m    111\u001b[0m output_paths \u001b[39m=\u001b[39m []\n\u001b[0;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m i, sample \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(output\u001b[39m.\u001b[39mimages):\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion_img2img.py:585\u001b[0m, in \u001b[0;36mStableDiffusionImg2ImgPipeline.__call__\u001b[1;34m(self, prompt, image, strength, num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, output_type, return_dict, callback, callback_steps, **kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     noise_pred \u001b[39m=\u001b[39m noise_pred_uncond \u001b[39m+\u001b[39m guidance_scale \u001b[39m*\u001b[39m (noise_pred_text \u001b[39m-\u001b[39m noise_pred_uncond)\n\u001b[0;32m    584\u001b[0m \u001b[39m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m latents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep(noise_pred, t, latents, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_step_kwargs)\u001b[39m.\u001b[39mprev_sample\n\u001b[0;32m    587\u001b[0m \u001b[39m# call the callback, if provided\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(timesteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m ((i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m>\u001b[39m num_warmup_steps \u001b[39mand\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39morder \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\diffusers\\schedulers\\scheduling_dpmsolver_multistep.py:457\u001b[0m, in \u001b[0;36mDPMSolverMultistepScheduler.step\u001b[1;34m(self, model_output, timestep, sample, return_dict)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(timestep, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    456\u001b[0m     timestep \u001b[39m=\u001b[39m timestep\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimesteps\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 457\u001b[0m step_index \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimesteps \u001b[39m==\u001b[39;49m timestep)\u001b[39m.\u001b[39;49mnonzero()\n\u001b[0;32m    458\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(step_index) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    459\u001b[0m     step_index \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimesteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" for i in range(5):\n",
    "    run_scr.predict(prompt = [\"\"], image=\"./c.jpg\", seed = int.from_bytes(os.urandom(2), \"big\"), output_location =\"./\" , index = i,\n",
    "    prompt_strength= 0.3, guidance_scale= 10.5, num_inference_steps=15) \"\"\"\n",
    "# str = 0.7 and scale = 2\n",
    "\n",
    "if EXTERNAL:\n",
    "    cont = 0\n",
    "    if EXTERNAL:\n",
    "        if (not os.path.exists(f\"{project_folder}\\\\style\\\\\")):\n",
    "            os.makedirs(f\"{project_folder}\\\\style\\\\\")\n",
    "        for folder_name in os.listdir(os.path.join(f\"{project_folder}\\\\img\\\\\")):\n",
    "            print(folder_name)\n",
    "            for filename in os.listdir(os.path.join(f\"{project_folder}\\\\img\\\\{folder_name}\")):\n",
    "                if (not os.path.exists(f\"{project_folder}\\\\style\\\\{folder_name}\")):\n",
    "                        os.makedirs(f\"{project_folder}\\\\style\\\\{folder_name}\")\n",
    "                if (filename == \"c.jpg\"):\n",
    "                    for i in range(IMG_NUMBER_IMG2IMG):\n",
    "                        run_scr.predict(prompt = \"\", \n",
    "                            image=f\"{project_folder}\\\\img\\\\{folder_name}\\\\{filename}\", seed = int.from_bytes(os.urandom(2), \"big\"),\n",
    "                            output_location =f\"{project_folder}\\\\style\\\\{folder_name}\" , index = i,\n",
    "                            prompt_strength= 0.4, guidance_scale = 4, num_inference_steps=15\n",
    "                        )\n",
    "else:\n",
    "    for i in range(100):\n",
    "        run_scr.predict(prompt = \"(((beautiful))), detailed, (((young woman))), high definition, realistic\", \n",
    "        image=\"./c.jpg\", seed = int.from_bytes(os.urandom(2), \"big\"), output_location =\"./random\" , index = i,\n",
    "        prompt_strength= 0.6, guidance_scale = 7.5, num_inference_steps=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cb0dd519e46c84f14fc2d0d6172fde6407ecb78b6d95b871c78761f2e6e52fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
