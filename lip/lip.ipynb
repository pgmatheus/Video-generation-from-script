{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir, path\n",
    "import numpy as np\n",
    "import scipy, cv2, os, sys, argparse, audio\n",
    "import json, subprocess, random, string\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import torch, face_detection\n",
    "from models import Wav2Lip\n",
    "import platform\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip, ImageSequenceClip, CompositeAudioClip\n",
    "# hd after\n",
    "\"\"\" from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from gfpgan import GFPGANer\n",
    "from realesrgan import RealESRGANer \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EXTERNAL = False\n",
    "ANIME = True\n",
    "\n",
    "if EXTERNAL:\n",
    "    args = {\n",
    "        'checkpoint_path': '.\\\\lip\\\\checkpoints\\\\wav2lip_gan.pth',\n",
    "        'face':'.\\\\result2.mp4',       \n",
    "        'temp_folder': '.\\\\lip\\\\temp\\\\result.mp4',\n",
    "        'static': False,\n",
    "        'fps': 30.0,\n",
    "        'pads':[0, 10, 0, 0],\n",
    "        'face_det_batch_size':16,\n",
    "        'wav2lip_batch_size':128,\n",
    "        'resize_factor':1,\n",
    "        'crop':[0, -1, 0, -1],\n",
    "        'box':[-1, -1, -1, -1],\n",
    "        'rotate': False,\n",
    "        'nosmooth':False,\n",
    "        'img_size': 96 \n",
    "    }\n",
    "    %store -r WIDTH\n",
    "    %store -r HEIGHT\n",
    "    %store -r TEXT\n",
    "    %store -r VISUALIZE\n",
    "    %store -r SAVE_FOLDER\n",
    "    %store -r PROJECT_NAME\n",
    "    %store -r SHOW_OUTPUT\n",
    "    %store -r IMG_NUMBER\n",
    "    %store -r THUMBNAIL\n",
    "    %store -r SKIP_VIDEO\n",
    "    %store -r MERGE_VIDEO\n",
    "    %store -r TEXT_VOICE_GEN\n",
    "    %store -r LANGUAGES\n",
    "    %store -r GEN_VIDEO\n",
    "    %store -r INSERT_BACKGROUND\n",
    "    %store -r RECORD_FRAME_INTER_AFTER\n",
    "else:\n",
    "\n",
    "\n",
    "    args = {\n",
    "        'checkpoint_path': '.\\\\lip\\\\checkpoints\\\\wav2lip_gan.pth',        \n",
    "        #'face':'.\\\\lip\\\\input_video.mp4',\n",
    "        'face':'.\\\\result2.mp4',\n",
    "        'audio':'.\\\\lip\\\\input_audio.wav',\n",
    "        'outfile': '.\\\\final.mp4',\n",
    "        'temp_folder': '.\\\\lip\\\\temp\\\\result.mp4',\n",
    "        'static': False,\n",
    "        'fps': 30.0,\n",
    "        'pads':[0, 10, 0, 0],\n",
    "        'face_det_batch_size':16,\n",
    "        'wav2lip_batch_size':128,\n",
    "        'resize_factor':1,\n",
    "        'crop':[0, -1, 0, -1],        \n",
    "        'box':[-1, -1, -1, -1],\n",
    "        'rotate': False,\n",
    "        'nosmooth':False,\n",
    "        'img_size': 96 \n",
    "    }\n",
    "    WIDTH = 768\n",
    "    HEIGHT = 768\n",
    "    VISUALIZE = True  \n",
    "    SAVE_FOLDER = \"D:\\\\Deletar\\\\p_gen\"\n",
    "    PROJECT_NAME = \"THE FELLOWSHIP OF THE RING\"\n",
    "    STYLES_FOLDER = \".\\\\styles\"\n",
    "    SHOW_OUTPUT = True\n",
    "    IMG_NUMBER = 2\n",
    "    SKIP_VIDEO = []\n",
    "    MERGE_VIDEO = False\n",
    "    TEXT_VOICE_GEN = ['A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A','A']\n",
    "    LANGUAGES = ['pt']\n",
    "    GEN_VIDEO = True\n",
    "    INSERT_BACKGROUND = False\n",
    "    RECORD_FRAME_INTER_AFTER = 0\n",
    "    RECORD_FRAME_LIP = -1\n",
    "\n",
    "\n",
    "mel_step_size = 16\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} for inference.'.format(device))\n",
    "\n",
    "\n",
    "project_folder = f\"{SAVE_FOLDER}//{PROJECT_NAME}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from: .\\checkpoints\\wav2lip_gan.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' netscale = 4\\nmodel_path_face = \"./realesrgan/GFPGANv1.3.pth\"\\ndni_weight = None\\ntile = 0\\ntitle_pad = 10\\npre_pad = 0\\nhalf = None\\ngpu_id = None\\nupscale = 3.5\\nupscale = 4\\nimg_mode = \\'RGBA\\'\\n\\nif ANIME:\\n  # R-ESRGAN + Anime\\n  model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\\n  model_path_x4 = \"./realesrgan/RealESRGAN_x4plus_anime_6B.pth\"\\nelse:\\n  # R-ESRGAN\\n  model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\\n  model_path_x4 = \"./realesrgan/RealESRGAN_x4plus.pth\"\\n\\nupsampler = RealESRGANer(\\n        scale=netscale,\\n        model_path=model_path_x4,\\n        dni_weight=dni_weight,\\n        model=model,\\n        tile=tile,\\n        tile_pad=title_pad,\\n        pre_pad=pre_pad,\\n        half= not half,\\n        gpu_id= gpu_id\\n)\\n\\nface_enhancer = GFPGANer(\\n            model_path=model_path_face,\\n            upscale=upscale,\\n            arch=\\'clean\\',\\n            channel_multiplier=2,\\n            bg_upsampler=upsampler) '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_smoothened_boxes(boxes, T):\n",
    "\tfor i in range(len(boxes)):\n",
    "\t\tif i + T > len(boxes):\n",
    "\t\t\twindow = boxes[len(boxes) - T:]\n",
    "\t\telse:\n",
    "\t\t\twindow = boxes[i : i + T]\n",
    "\t\tboxes[i] = np.mean(window, axis=0)\n",
    "\treturn boxes\n",
    "\n",
    "def face_detect(images):\n",
    "\tdetector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \n",
    "\t\t\t\t\t\t\t\t\t\t\tflip_input=False, device=device)\n",
    "\n",
    "\tbatch_size = args['face_det_batch_size']\n",
    "\t\n",
    "\twhile 1:\n",
    "\t\tpredictions = []\n",
    "\t\ttry:\n",
    "\t\t\tfor i in tqdm(range(0, len(images), batch_size)):\n",
    "\t\t\t\tpredictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\n",
    "\t\texcept RuntimeError:\n",
    "\t\t\tif batch_size == 1: \n",
    "\t\t\t\traise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')\n",
    "\t\t\tbatch_size //= 2\n",
    "\t\t\tprint('Recovering from OOM error; New batch size: {}'.format(batch_size))\n",
    "\t\t\tcontinue\n",
    "\t\tbreak\n",
    "\n",
    "\tresults = []\n",
    "\tpady1, pady2, padx1, padx2 = args['pads']\n",
    "\tfor rect, image in zip(predictions, images):\n",
    "\t\tif rect is None:\n",
    "\t\t\tcv2.imwrite('temp/faulty_frame.jpg', image) # check this frame where the face was not detected.\n",
    "\t\t\traise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n",
    "\n",
    "\t\ty1 = max(0, rect[1] - pady1)\n",
    "\t\ty2 = min(image.shape[0], rect[3] + pady2)\n",
    "\t\tx1 = max(0, rect[0] - padx1)\n",
    "\t\tx2 = min(image.shape[1], rect[2] + padx2)\n",
    "\t\t\n",
    "\t\tresults.append([x1, y1, x2, y2])\n",
    "\n",
    "\tboxes = np.array(results)\n",
    "\tif not args['nosmooth']: boxes = get_smoothened_boxes(boxes, T=5)\n",
    "\tresults = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n",
    "\n",
    "\tdel detector\n",
    "\treturn results \n",
    "\n",
    "def datagen(frames, mels):\n",
    "\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
    "\n",
    "\tif args['box'][0] == -1:\n",
    "\t\tif not args['static']:\n",
    "\t\t\tface_det_results = face_detect(frames) # BGR2RGB for CNN face detection\n",
    "\t\telse:\n",
    "\t\t\tface_det_results = face_detect([frames[0]])\n",
    "\telse:\n",
    "\t\tprint('Using the specified bounding box instead of face detection...')\n",
    "\t\ty1, y2, x1, x2 = args['box']\n",
    "\t\tface_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in frames]\n",
    "\n",
    "\tfor i, m in enumerate(mels):\n",
    "\t\tidx = 0 if args['static'] else i%len(frames)\n",
    "\t\tframe_to_save = frames[idx].copy()\n",
    "\t\tface, coords = face_det_results[idx].copy()\n",
    "\n",
    "\t\tface = cv2.resize(face, (args['img_size'], args['img_size']))\n",
    "\t\t\t\n",
    "\t\timg_batch.append(face)\n",
    "\t\tmel_batch.append(m)\n",
    "\t\tframe_batch.append(frame_to_save)\n",
    "\t\tcoords_batch.append(coords)\n",
    "\n",
    "\t\tif len(img_batch) >= args['wav2lip_batch_size']:\n",
    "\t\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
    "\n",
    "\t\t\timg_masked = img_batch.copy()\n",
    "\t\t\timg_masked[:, args['img_size']//2:] = 0\n",
    "\n",
    "\t\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
    "\t\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "\t\t\tyield img_batch, mel_batch, frame_batch, coords_batch\n",
    "\t\t\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
    "\n",
    "\tif len(img_batch) > 0:\n",
    "\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
    "\n",
    "\t\timg_masked = img_batch.copy()\n",
    "\t\timg_masked[:, args['img_size']//2:] = 0\n",
    "\n",
    "\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
    "\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "\t\tyield img_batch, mel_batch, frame_batch, coords_batch\n",
    "\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "\tif device == 'cuda':\n",
    "\t\tcheckpoint = torch.load(checkpoint_path)\n",
    "\telse:\n",
    "\t\tcheckpoint = torch.load(checkpoint_path,\n",
    "\t\t\t\t\t\t\t\tmap_location=lambda storage, loc: storage)\n",
    "\treturn checkpoint\n",
    "\n",
    "def load_model(path):\n",
    "\tmodel = Wav2Lip()\n",
    "\tprint(\"Load checkpoint from: {}\".format(path))\n",
    "\tcheckpoint = _load(path)\n",
    "\ts = checkpoint[\"state_dict\"]\n",
    "\tnew_s = {}\n",
    "\tfor k, v in s.items():\n",
    "\t\tnew_s[k.replace('module.', '')] = v\n",
    "\tmodel.load_state_dict(new_s)\n",
    "\n",
    "\tmodel = model.to(device)\n",
    "\treturn model.eval()\n",
    "\n",
    "model = load_model(args['checkpoint_path'])\n",
    "\n",
    "project_folder = f\"{SAVE_FOLDER}//{PROJECT_NAME}\"\n",
    "\n",
    "\"\"\" netscale = 4\n",
    "model_path_face = \"./realesrgan/GFPGANv1.3.pth\"\n",
    "dni_weight = None\n",
    "tile = 0\n",
    "title_pad = 10\n",
    "pre_pad = 0\n",
    "half = None\n",
    "gpu_id = None\n",
    "upscale = 3.5\n",
    "upscale = 4\n",
    "img_mode = 'RGBA'\n",
    "\n",
    "if ANIME:\n",
    "  # R-ESRGAN + Anime\n",
    "  model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n",
    "  model_path_x4 = \"./realesrgan/RealESRGAN_x4plus_anime_6B.pth\"\n",
    "else:\n",
    "  # R-ESRGAN\n",
    "  model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
    "  model_path_x4 = \"./realesrgan/RealESRGAN_x4plus.pth\"\n",
    "\n",
    "upsampler = RealESRGANer(\n",
    "        scale=netscale,\n",
    "        model_path=model_path_x4,\n",
    "        dni_weight=dni_weight,\n",
    "        model=model,\n",
    "        tile=tile,\n",
    "        tile_pad=title_pad,\n",
    "        pre_pad=pre_pad,\n",
    "        half= not half,\n",
    "        gpu_id= gpu_id\n",
    ")\n",
    "\n",
    "face_enhancer = GFPGANer(\n",
    "            model_path=model_path_face,\n",
    "            upscale=upscale,\n",
    "            arch='clean',\n",
    "            channel_multiplier=2,\n",
    "            bg_upsampler=upsampler) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lip_sync(video_location, audio_location, output_folder):\n",
    "\targs['face'] = video_location\n",
    "\targs['audio'] = audio_location\n",
    "\n",
    "\n",
    "\tif os.path.isfile(args['face']) and args['face'].split('.')[1] in ['jpg', 'png', 'jpeg']:\n",
    "\t\targs['static'] = True\n",
    "\n",
    "\tif not os.path.isfile(args['face']):\n",
    "\t\traise ValueError('--face argument must be a valid path to video/image file')\n",
    "\n",
    "\telif args['face'].split('.')[1] in ['jpg', 'png', 'jpeg']:\n",
    "\t\tfull_frames = [cv2.imread(args['face'])]\n",
    "\t\tfps = args['fps']\n",
    "\n",
    "\telse:\n",
    "\t\tvideo_stream = cv2.VideoCapture(args['face'])\n",
    "\t\tfps = video_stream.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\t\tprint('Reading video frames...')\n",
    "\n",
    "\t\tfull_frames = []\n",
    "\t\twhile 1:\n",
    "\t\t\tstill_reading, frame = video_stream.read()\n",
    "\t\t\tif not still_reading:\n",
    "\t\t\t\tvideo_stream.release()\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif args['resize_factor'] > 1:\n",
    "\t\t\t\tframe = cv2.resize(frame, (frame.shape[1]//args['resize_factor'], frame.shape[0]//args['resize_factor']))\n",
    "\n",
    "\t\t\tif args['rotate']:\n",
    "\t\t\t\tframe = cv2.rotate(frame, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "\t\t\ty1, y2, x1, x2 = args['crop']\n",
    "\t\t\tif x2 == -1: x2 = frame.shape[1]\n",
    "\t\t\tif y2 == -1: y2 = frame.shape[0]\n",
    "\n",
    "\t\t\tframe = frame[y1:y2, x1:x2]\n",
    "\n",
    "\t\t\tfull_frames.append(frame)\n",
    "\n",
    "\tprint (\"Number of frames available for inference: \"+str(len(full_frames)))\n",
    "\n",
    "\tif not args['audio'].endswith('.wav'):\n",
    "\t\tprint('Extracting raw audio...')\n",
    "\t\tcommand = 'ffmpeg -y -i {} -strict -2 {}'.format(args['audio'], 'temp/temp.wav')\n",
    "\n",
    "\t\tsubprocess.call(command, shell=True)\n",
    "\t\targs['audio'] = '.\\\\lip\\\\temp\\\\temp.wav'\n",
    "\n",
    "\twav = audio.load_wav(args['audio'], 16000)\n",
    "\tmel = audio.melspectrogram(wav)\n",
    "\tprint(mel.shape)\n",
    "\n",
    "\tif np.isnan(mel.reshape(-1)).sum() > 0:\n",
    "\t\traise ValueError('Mel contains nan! Using a TTS voice? Add a small epsilon noise to the wav file and try again')\n",
    "\n",
    "\tmel_chunks = []\n",
    "\tmel_idx_multiplier = 80./fps \n",
    "\ti = 0\n",
    "\twhile 1:\n",
    "\t\tstart_idx = int(i * mel_idx_multiplier)\n",
    "\t\tif start_idx + mel_step_size > len(mel[0]):\n",
    "\t\t\tmel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n",
    "\t\t\tbreak\n",
    "\t\tmel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n",
    "\t\ti += 1\n",
    "\n",
    "\tprint(\"Length of mel chunks: {}\".format(len(mel_chunks)))\n",
    "\n",
    "\tfull_frames = full_frames[:len(mel_chunks)]\n",
    "\n",
    "\tbatch_size = args['wav2lip_batch_size']\n",
    "\tgen = datagen(full_frames.copy(), mel_chunks)\n",
    "\n",
    "\tfor i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen, \n",
    "\t\t\t\t\t\t\t\t\t\t\ttotal=int(np.ceil(float(len(mel_chunks))/batch_size)))):\n",
    "\t\tif i == 0:\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tframe_h, frame_w = full_frames[0].shape[:-1]\n",
    "\t\t\t\"\"\" out = cv2.VideoWriter('temp/result.avi', \n",
    "\t\t\t\t\t\t\t\t\tcv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h)) \"\"\"\n",
    "\t\t\tout = cv2.VideoWriter(args['temp_folder'], \n",
    "\t\t\t\t\t\t\t\t\tcv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\n",
    "\n",
    "\t\timg_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\n",
    "\t\tmel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred = model(mel_batch, img_batch)\n",
    "\n",
    "\t\tpred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
    "\t\t\n",
    "\t\tfor p, f, c in zip(pred, frames, coords):\n",
    "\t\t\ty1, y2, x1, x2 = c\n",
    "\t\t\tp = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n",
    "\n",
    "\t\t\tf[y1:y2, x1:x2] = p\n",
    "\t\t\tout.write(f)\n",
    "\n",
    "\tout.release()\n",
    "\n",
    "\taudio1 = AudioFileClip(args['audio'])\n",
    "\tvideo = VideoFileClip(args['temp_folder'])   \n",
    "\tfinal_video = video.set_audio(audio1)\n",
    "\tif os.path.exists(output_folder):\n",
    "\t\tos.remove(output_folder) \n",
    "\tfinal_video.write_videofile(output_folder)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading video frames...\n",
      "Number of frames available for inference: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\lip\\\\input_audio.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\librosa\\core\\audio.py:164\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     y, sr_native \u001b[39m=\u001b[39m __soundfile_load(path, offset, duration, dtype)\n\u001b[0;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    167\u001b[0m     \u001b[39m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\librosa\\core\\audio.py:195\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m     \u001b[39m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m     context \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39;49mSoundFile(path)\n\u001b[0;32m    197\u001b[0m \u001b[39mwith\u001b[39;00m context \u001b[39mas\u001b[39;00m sf_desc:\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\soundfile.py:655\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info \u001b[39m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    654\u001b[0m                                  \u001b[39mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(file, mode_int, closefd)\n\u001b[0;32m    656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(mode)\u001b[39m.\u001b[39missuperset(\u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n\u001b[0;32m    657\u001b[0m     \u001b[39m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\soundfile.py:1213\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     err \u001b[39m=\u001b[39m _snd\u001b[39m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1213\u001b[0m     \u001b[39mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError opening \u001b[39m\u001b[39m{0!r}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname))\n\u001b[0;32m   1214\u001b[0m \u001b[39mif\u001b[39;00m mode_int \u001b[39m==\u001b[39m _snd\u001b[39m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1215\u001b[0m     \u001b[39m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m     \u001b[39m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m     \u001b[39m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening '.\\\\lip\\\\input_audio.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 77\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" if (not os.path.exists(f\"{project_folder}//lip_hd\")):\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m                    os.makedirs(f\"{project_folder}//lip_hd\")\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39m    for folder in os.listdir(f\"{project_folder}//lip\"):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m        os.remove(f\"{project_folder}//full//full_lip.mp4\")\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m    merged_video.write_videofile(f\"{project_folder}//full//full_lip.mp4\") \"\"\"\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m      lip_sync(args[\u001b[39m'\u001b[39;49m\u001b[39mface\u001b[39;49m\u001b[39m'\u001b[39;49m], args[\u001b[39m'\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m'\u001b[39;49m], args[\u001b[39m'\u001b[39;49m\u001b[39moutfile\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m, in \u001b[0;36mlip_sync\u001b[1;34m(video_location, audio_location, output_folder)\u001b[0m\n\u001b[0;32m     48\u001b[0m \tsubprocess\u001b[39m.\u001b[39mcall(command, shell\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m \targs[\u001b[39m'\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mlip\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtemp\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtemp.wav\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 51\u001b[0m wav \u001b[39m=\u001b[39m audio\u001b[39m.\u001b[39;49mload_wav(args[\u001b[39m'\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m16000\u001b[39;49m)\n\u001b[0;32m     52\u001b[0m mel \u001b[39m=\u001b[39m audio\u001b[39m.\u001b[39mmelspectrogram(wav)\n\u001b[0;32m     53\u001b[0m \u001b[39mprint\u001b[39m(mel\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mf:\\video_g\\lip\\audio.py:10\u001b[0m, in \u001b[0;36mload_wav\u001b[1;34m(path, sr)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_wav\u001b[39m(path, sr):\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m librosa\u001b[39m.\u001b[39;49mcore\u001b[39m.\u001b[39;49mload(path, sr\u001b[39m=\u001b[39;49msr)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\librosa\\util\\decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[0;32m     87\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     91\u001b[0m args_msg \u001b[39m=\u001b[39m [\n\u001b[0;32m     92\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, arg)\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[39m-\u001b[39mextra_args:])\n\u001b[0;32m     94\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\librosa\\core\\audio.py:170\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, (\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPurePath)):\n\u001b[0;32m    169\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[39m\"\u001b[39m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m--> 170\u001b[0m     y, sr_native \u001b[39m=\u001b[39m __audioread_load(path, offset, duration, dtype)\n\u001b[0;32m    171\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    172\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\librosa\\core\\audio.py:226\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    223\u001b[0m     reader \u001b[39m=\u001b[39m path\n\u001b[0;32m    224\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[39m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     reader \u001b[39m=\u001b[39m audioread\u001b[39m.\u001b[39;49maudio_open(path)\n\u001b[0;32m    228\u001b[0m \u001b[39mwith\u001b[39;00m reader \u001b[39mas\u001b[39;00m input_file:\n\u001b[0;32m    229\u001b[0m     sr_native \u001b[39m=\u001b[39m input_file\u001b[39m.\u001b[39msamplerate\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\audioread\\__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m BackendClass \u001b[39min\u001b[39;00m backends:\n\u001b[0;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m BackendClass(path)\n\u001b[0;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m DecodeError:\n\u001b[0;32m    129\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\anaconda3\\envs\\Pyflow\\lib\\site-packages\\audioread\\rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, filename):\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fh \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     61\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m aifc\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fh)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\lip\\\\input_audio.wav'"
     ]
    }
   ],
   "source": [
    "if EXTERNAL:\n",
    "       \n",
    "    video = []\n",
    "    cont = -1\n",
    "    start_time = 0\n",
    "    acc_time = 0\n",
    "    video_face_lq = VideoFileClip(args['face'])    \n",
    "    if (not os.path.exists(f\"{project_folder}//lip\")):\n",
    "                    os.makedirs(f\"{project_folder}//lip\")\n",
    "    for folder in os.listdir(f\"{project_folder}//video_sound_hd\"):\n",
    "        for filename in os.listdir(f\"{project_folder}//video_sound_hd//{folder}\"):\n",
    "            if (filename[-3:] == 'mp4'):\n",
    "                cont = cont + 1\n",
    "                addr_video = f\"{project_folder}//video_sound_hd//{folder}//{filename}\"\n",
    "                video_hq = VideoFileClip(addr_video)                    \n",
    "                original_audio = video_hq.audio\n",
    "                original_audio_duration = round(original_audio.duration,3)\n",
    "\n",
    "                if (RECORD_FRAME_LIP < cont):\n",
    "                    print(filename[:-4])                    \n",
    "                    if (not os.path.exists(f\"{project_folder}//lip//{filename[:-4]}\")):\n",
    "                        os.makedirs(f\"{project_folder}//lip//{filename[:-4]}\")\n",
    "                    subclip_face = video_face_lq.subclip(acc_time, acc_time + original_audio_duration)\n",
    "                    temp_video_face_file_address = f'.\\\\lip\\\\temp\\\\{filename[:-4]}.mp4'                    \n",
    "                    \"\"\" if os.path.exists(temp_video_face_file_address):\n",
    "                        os.remove(temp_video_face_file_address)  \"\"\"                                   \n",
    "                    temp_audio_file_address = f'.\\\\lip\\\\temp\\\\{filename[:-4]}.wav'\n",
    "                    \"\"\" if os.path.exists(temp_audio_file_address):\n",
    "                        os.remove(temp_audio_file_address) \"\"\"\n",
    "                    subclip_face.write_videofile(temp_video_face_file_address) \n",
    "                    original_audio.write_audiofile(temp_audio_file_address)\n",
    "                    if os.path.exists(f\"{project_folder}//lip//{filename[:-4]}//{filename[:-4]}.mp4\"):\n",
    "                        os.remove(f\"{project_folder}//lip//{filename[:-4]}//{filename[:-4]}.mp4\")\n",
    "                    print(f\"{project_folder}//lip//{filename[:-4]}//{filename[:-4]}.mp4\")\n",
    "                    print(temp_audio_file_address)\n",
    "                    print(temp_video_face_file_address)\n",
    "                    lip_sync(temp_video_face_file_address, temp_audio_file_address, f\"{project_folder}//lip//{filename[:-4]}//{filename[:-4]}.mp4\")\n",
    "                    os.remove(temp_video_face_file_address)\n",
    "                    os.remove(temp_audio_file_address)                 \n",
    "                acc_time = round(acc_time + original_audio_duration,3)\n",
    "    \"\"\" if (not os.path.exists(f\"{project_folder}//lip_hd\")):\n",
    "                    os.makedirs(f\"{project_folder}//lip_hd\")\n",
    "    for folder in os.listdir(f\"{project_folder}//lip\"):\n",
    "        for filename in os.listdir(f\"{project_folder}//lip//{folder}\"):                   \n",
    "            if (filename[-3:] == 'mp4'):\n",
    "                frames_hd = []\n",
    "                addr_video = f\"{project_folder}//lip//{folder}//{filename}\"                \n",
    "                video_lq_lip = VideoFileClip(addr_video)  \n",
    "                audio_lq_lip = video_lq_lip.audio        \n",
    "                if (not os.path.exists(f\"{project_folder}//lip_hd//{folder}\")):\n",
    "                    os.makedirs(f\"{project_folder}//lip_hd//{folder}\")           \n",
    "                for i, frame in enumerate(video_lq_lip.iter_frames()):                    \n",
    "                    _, _, output = face_enhancer.enhance(frame, has_aligned=False, only_center_face=False, paste_back=True)\n",
    "                    frames_hd.append(output)\n",
    "                if (not os.path.exists(f\"{project_folder}//lip_hd//{filename[:-4]}\")):\n",
    "                    os.makedirs(f\"{project_folder}//lip_hd//{filename[:-4]}\")\n",
    "                if os.path.exists(f\"{project_folder}//lip_hd//{filename[:-4]}//{filename[:-4]}.mp4\"):\n",
    "                        os.remove(f\"{project_folder}//lip_hd//{filename[:-4]}//{filename[:-4]}.mp4\")   \n",
    "                video_hd_lip = ImageSequenceClip(frames_hd, fps=30)\n",
    "                video_hd_lip = video_hd_lip.set_audio(audio_lq_lip)                \n",
    "                video_hd_lip.write_videofile(f\"{project_folder}//lip_hd//{filename[:-4]}//{filename[:-4]}.mp4\")\n",
    "    for folder in os.listdir(f\"{project_folder}//lip_hd\"):\n",
    "        for filename in os.listdir(f\"{project_folder}//lip_hd//{folder}\"):\n",
    "             if (filename[-3:] == 'mp4'):\n",
    "                  temp_lip_video = VideoFileClip(f\"{project_folder}//lip_hd//{folder}//{filename}\")\n",
    "                  video.append(temp_lip_video)\n",
    "    merged_video = concatenate_videoclips(video, method='chain')\n",
    "    if os.path.exists(f\"{project_folder}//full//full_lip.mp4\"):\n",
    "        os.remove(f\"{project_folder}//full//full_lip.mp4\")\n",
    "    merged_video.write_videofile(f\"{project_folder}//full//full_lip.mp4\") \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "     lip_sync(args['face'], args['audio'], args['outfile'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:16:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cb0dd519e46c84f14fc2d0d6172fde6407ecb78b6d95b871c78761f2e6e52fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
